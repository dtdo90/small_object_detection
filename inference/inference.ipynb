{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REVYd66jcwZX",
        "outputId": "d27c29c0-49b8-4030-94de-39c55122e903"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IV-EKfRPc5pY",
        "outputId": "ef00e1f7-8d62-4cbf-c67f-f76c921d4acb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/yolov8-tensorrt\n"
          ]
        }
      ],
      "source": [
        "%cd '/content/drive/MyDrive/yolov8-tensorrt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CutsNHzddER",
        "outputId": "f5c5a949-f21a-404a-802f-c41378948e2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorrt (from -r requirements.txt (line 1))\n",
            "  Downloading tensorrt-10.7.0.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorrt_lean (from -r requirements.txt (line 2))\n",
            "  Downloading tensorrt_lean-10.7.0.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorrt_dispatch (from -r requirements.txt (line 3))\n",
            "  Downloading tensorrt_dispatch-10.7.0.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting onnx (from -r requirements.txt (line 4))\n",
            "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting onnxsim (from -r requirements.txt (line 5))\n",
            "  Downloading onnxsim-0.4.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Collecting onnxruntime-gpu (from -r requirements.txt (line 6))\n",
            "  Downloading onnxruntime_gpu-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting tensorrt_cu12==10.7.0 (from tensorrt->-r requirements.txt (line 1))\n",
            "  Downloading tensorrt_cu12-10.7.0.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorrt_lean_cu12==10.7.0 (from tensorrt_lean->-r requirements.txt (line 2))\n",
            "  Downloading tensorrt_lean_cu12-10.7.0.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorrt_dispatch_cu12==10.7.0 (from tensorrt_dispatch->-r requirements.txt (line 3))\n",
            "  Downloading tensorrt_dispatch_cu12-10.7.0.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx->-r requirements.txt (line 4)) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx->-r requirements.txt (line 4)) (4.25.5)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from onnxsim->-r requirements.txt (line 5)) (13.9.4)\n",
            "Collecting coloredlogs (from onnxruntime-gpu->-r requirements.txt (line 6))\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu->-r requirements.txt (line 6)) (24.3.25)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu->-r requirements.txt (line 6)) (24.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu->-r requirements.txt (line 6)) (1.13.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu->-r requirements.txt (line 6))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->onnxsim->-r requirements.txt (line 5)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->onnxsim->-r requirements.txt (line 5)) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from rich->onnxsim->-r requirements.txt (line 5)) (4.12.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime-gpu->-r requirements.txt (line 6)) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->onnxsim->-r requirements.txt (line 5)) (0.1.2)\n",
            "Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxsim-0.4.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime_gpu-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (291.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m291.5/291.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: tensorrt, tensorrt_cu12, tensorrt_lean, tensorrt_lean_cu12, tensorrt_dispatch, tensorrt_dispatch_cu12\n",
            "  Building wheel for tensorrt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt: filename=tensorrt-10.7.0-py2.py3-none-any.whl size=16336 sha256=4a42dcddff587c2b709fe4576de79c059be2e23af8365532057cb8caf6839969\n",
            "  Stored in directory: /root/.cache/pip/wheels/da/cb/16/d5add64df498ec418cc9eb2885dc828a67a002afc30873d932\n",
            "  Building wheel for tensorrt_cu12 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt_cu12: filename=tensorrt_cu12-10.7.0-py2.py3-none-any.whl size=17551 sha256=6f3afe9c23e2f08d72638785daef3a581be3a55965540d3fbc9899c7f33739e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/dd/9d/413a390ab4b9ebf16701f91cecf9d94a2d481ea2949bcd72e9\n",
            "  Building wheel for tensorrt_lean (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt_lean: filename=tensorrt_lean-10.7.0-py2.py3-none-any.whl size=16409 sha256=0dea31268588d8df38432f825e632607d360337144d01a5a40a85486efffd8be\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/a7/de/fb27625c8546736e9b1a0987036a1d2a4cf2c2cb60e783344f\n",
            "  Building wheel for tensorrt_lean_cu12 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt_lean_cu12: filename=tensorrt_lean_cu12-10.7.0-py2.py3-none-any.whl size=17634 sha256=ee7e0e8205abbe7ad598f432edf695f1b565d7291fa513b4f597de0548f24933\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/e6/a5/a281da5dbb00fe09b37b147a6866bf8ae519b04306157a5637\n",
            "  Building wheel for tensorrt_dispatch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt_dispatch: filename=tensorrt_dispatch-10.7.0-py2.py3-none-any.whl size=16463 sha256=8413a90d9f5410674960d8a27e36c872314d5139ca037bb719b1dbd3a7e1184a\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/ba/46/5b839a443b760de566d40c9bbf619a300a860adc5c48b8f4be\n",
            "  Building wheel for tensorrt_dispatch_cu12 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt_dispatch_cu12: filename=tensorrt_dispatch_cu12-10.7.0-py2.py3-none-any.whl size=17702 sha256=961274f8ef78a0bcb104439b53207ab4baf4e1b67614e22b5e96316430388eb7\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/d0/3f/cf520c8c06e4d1ad5d806fd3985f83e8cbf4deee94845f402e\n",
            "Successfully built tensorrt tensorrt_cu12 tensorrt_lean tensorrt_lean_cu12 tensorrt_dispatch tensorrt_dispatch_cu12\n",
            "Installing collected packages: tensorrt_lean_cu12, tensorrt_dispatch_cu12, tensorrt_cu12, onnx, humanfriendly, tensorrt_lean, tensorrt_dispatch, tensorrt, coloredlogs, onnxsim, onnxruntime-gpu\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.17.0 onnxruntime-gpu-1.20.1 onnxsim-0.4.36 tensorrt-10.7.0 tensorrt_cu12-10.7.0 tensorrt_dispatch-10.7.0 tensorrt_dispatch_cu12-10.7.0 tensorrt_lean-10.7.0 tensorrt_lean_cu12-10.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRPvFmAQdfIM",
        "outputId": "eecd3edb-f45f-4866-bf81-534ef9297401"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics 8.3.58 üöÄ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "YOLO11l summary (fused): 464 layers, 25,287,022 parameters, 0 gradients, 86.6 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo_11l_469.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 14, 8400) (48.8 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.46...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 5.1s, saved as 'yolo_11l_469.onnx' (96.8 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.7.0...\n",
            "[01/07/2025-02:24:12] [TRT] [I] [MemUsageChange] Init CUDA: CPU -2, GPU +0, now: CPU 1299, GPU 1123 (MiB)\n",
            "[01/07/2025-02:24:14] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +955, GPU +194, now: CPU 2178, GPU 1317 (MiB)\n",
            "[01/07/2025-02:24:14] [TRT] [I] ----------------------------------------------------------------\n",
            "[01/07/2025-02:24:14] [TRT] [I] Input filename:   yolo_11l_469.onnx\n",
            "[01/07/2025-02:24:14] [TRT] [I] ONNX IR version:  0.0.9\n",
            "[01/07/2025-02:24:14] [TRT] [I] Opset version:    19\n",
            "[01/07/2025-02:24:14] [TRT] [I] Producer name:    pytorch\n",
            "[01/07/2025-02:24:14] [TRT] [I] Producer version: 2.5.1\n",
            "[01/07/2025-02:24:14] [TRT] [I] Domain:           \n",
            "[01/07/2025-02:24:14] [TRT] [I] Model version:    0\n",
            "[01/07/2025-02:24:14] [TRT] [I] Doc string:       \n",
            "[01/07/2025-02:24:14] [TRT] [I] ----------------------------------------------------------------\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 640, 640) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 14, 8400) DataType.FLOAT\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP16 engine as yolo_11l_469.engine\n",
            "[01/07/2025-02:24:14] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
            "[01/07/2025-02:24:14] [TRT] [I] BuilderFlag::kTF32 is set but hardware does not support TF32. Disabling TF32.\n",
            "[01/07/2025-02:24:14] [TRT] [I] Local timing cache in use. Profiling results in this builder pass will not be stored.\n",
            "[01/07/2025-02:27:39] [TRT] [I] Compiler backend is used during engine build.\n",
            "[01/07/2025-02:30:37] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
            "[01/07/2025-02:30:40] [TRT] [I] Total Host Persistent Memory: 946048 bytes\n",
            "[01/07/2025-02:30:40] [TRT] [I] Total Device Persistent Memory: 1762816 bytes\n",
            "[01/07/2025-02:30:40] [TRT] [I] Max Scratch Memory: 2764800 bytes\n",
            "[01/07/2025-02:30:40] [TRT] [I] [BlockAssignment] Started assigning block shifts. This will take 270 steps to complete.\n",
            "[01/07/2025-02:30:40] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 61.152ms to assign 12 blocks to 270 nodes requiring 37479936 bytes.\n",
            "[01/07/2025-02:30:40] [TRT] [I] Total Activation Memory: 37478400 bytes\n",
            "[01/07/2025-02:30:40] [TRT] [I] Total Weights Memory: 50721346 bytes\n",
            "[01/07/2025-02:30:40] [TRT] [I] Compiler backend is used during engine execution.\n",
            "[01/07/2025-02:30:40] [TRT] [I] Engine generation completed in 385.452 seconds.\n",
            "[01/07/2025-02:30:40] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 4 MiB, GPU 200 MiB\n",
            "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success ‚úÖ 393.5s, saved as 'yolo_11l_469.engine' (51.6 MB)\n",
            "\n",
            "Export complete (395.3s)\n",
            "Results saved to \u001b[1m/content/drive/MyDrive/yolov8-tensorrt\u001b[0m\n",
            "Predict:         yolo predict task=detect model=yolo_11l_469.engine imgsz=640 half \n",
            "Validate:        yolo val task=detect model=yolo_11l_469.engine imgsz=640 data=/notebooks/datasets/VisDrone/VisDrone.yaml half \n",
            "Visualize:       https://netron.app\n",
            "üí° Learn more at https://docs.ultralytics.com/modes/export\n"
          ]
        }
      ],
      "source": [
        "# TensorRT FP16\n",
        "from ultralytics import YOLO\n",
        "model = YOLO('yolo_11l_469.pt')\n",
        "model.export(format=\"engine\", half=True, device=0)\n",
        "# !yolo export model=yolo_11l_469.pt format=engine half=True device=0 workspace=12\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "XhgpKwrYetzT"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import time\n",
        "from utils import VisTrack\n",
        "from PIL import ImageFont\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "class InferenceBaseline:\n",
        "    def __init__(self,model_path):\n",
        "        self.detection_model=None\n",
        "        self.names={\n",
        "            \"0\": \"pedestrian\",\n",
        "            \"1\": \"people\",\n",
        "            \"2\": \"bicycle\",\n",
        "            \"3\": \"car\",\n",
        "            \"4\": \"van\",\n",
        "            \"5\": \"truck\",\n",
        "            \"6\": \"tricycle\",\n",
        "            \"7\": \"awning-tricycle\",\n",
        "            \"8\": \"bus\",\n",
        "            \"9\": \"motor\"\n",
        "        }\n",
        "        # utility function for drawing\n",
        "        self.vis_track = VisTrack()\n",
        "\n",
        "        # initialize the model\n",
        "        self.detection_model=YOLO(model_path)\n",
        "        # fuse pytorch model for faster inference\n",
        "        if model_path[-2:] == \"pt\":\n",
        "            self.detection_model.fuse()  # Fuse Conv2d + BatchNorm2d layers\n",
        "\n",
        "\n",
        "    def inference_image(self,img_path):\n",
        "        # read image\n",
        "        image=cv2.imread(img_path)\n",
        "        with torch.no_grad():  # Disable gradient calculation\n",
        "            results = self.detection_model.predict(image,conf=0.35,device=0)\n",
        "\n",
        "        results=results[0]  # results = list with 1 element\n",
        "        bboxes=results.boxes.xyxy.cpu().numpy()\n",
        "        ids=results.boxes.cls.cpu().numpy().astype(int)\n",
        "        scores=results.boxes.conf.cpu().numpy()\n",
        "\n",
        "        image=self.vis_track.draw_bounding_boxes(image,bboxes,ids,self.names,scores)\n",
        "        cv2.imwrite(img_path[:-4]+\"_yolo.png\",image)\n",
        "\n",
        "\n",
        "    def inference_video(self, video_path):\n",
        "\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        assert cap.isOpened(), \"Error reading video file\"\n",
        "\n",
        "        frame_count = 0\n",
        "        width, height, fps = int(cap.get(3)), int(cap.get(4)), int(cap.get(5))\n",
        "        num_frames = int(cap.get(7))\n",
        "        print(f\"Processing {num_frames} frames | Resolution: {width}x{height}\")\n",
        "        out = cv2.VideoWriter(video_path[:-4] + \"_processed.mp4\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps//2, (width, height))\n",
        "\n",
        "        start=time.time()\n",
        "\n",
        "        while True:\n",
        "            start_time = time.time()\n",
        "            success, frame = cap.read()\n",
        "            frame_count+=1\n",
        "\n",
        "            # Skip the frame if it fails to read\n",
        "            if not success:\n",
        "                print(f\"Warning: Skipping frame {frame_count + 1}\")\n",
        "                frame_count += 1\n",
        "                continue\n",
        "\n",
        "            frame_count += 1\n",
        "            with torch.no_grad():  # Disable gradient calculation\n",
        "              results = self.detection_model.predict(frame,conf=0.35,device=0,verbose=False)\n",
        "            results=results[0] # results = list with 1 element\n",
        "\n",
        "            boxes=results.boxes.xyxy.cpu().numpy()\n",
        "            ids=results.boxes.cls.cpu().numpy()\n",
        "            ids=ids.astype(int) # for suitability to VisTrack\n",
        "\n",
        "            scores=results.boxes.conf.cpu().numpy()\n",
        "\n",
        "            # Draw\n",
        "            frame_processed = self.vis_track.draw_bounding_boxes(frame, boxes, ids, self.names, scores)\n",
        "\n",
        "            # draw fps\n",
        "            end_time=time.time()\n",
        "            fps=1/(end_time-start_time)\n",
        "            cv2.putText(frame_processed, f\"FPS: {fps:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "            # write\n",
        "            out.write(frame_processed)\n",
        "\n",
        "            # Break when all frames are processed\n",
        "            if frame_count >= num_frames:\n",
        "                break\n",
        "        print(f\"Process finished in {(time.time()-start)/60:.2f} minutes\")\n",
        "        out.release()\n",
        "        cap.release()\n",
        "\n",
        "\n",
        "    def inference_webcam(self, idx):\n",
        "        if type(idx) is not int: # video input\n",
        "            raise ValueError(\"Camera index must be an integer\")\n",
        "        cap = cv2.VideoCapture(idx)\n",
        "\n",
        "        while True:\n",
        "            start_time = time.time()\n",
        "            success, frame = cap.read()\n",
        "            assert success, \"Fail to read frame\"\n",
        "\n",
        "            with torch.no_grad():  # Disable gradient calculation\n",
        "              results = self.detection_model.predict(frame,conf=0.35,device=0)\n",
        "            results=results[0] # results = list with 1 element\n",
        "\n",
        "            boxes=results.boxes.xyxy.cpu().numpy()\n",
        "            ids=results.boxes.cls.cpu().numpy()\n",
        "            ids=ids.astype(int) # for suitability to VisTrack\n",
        "\n",
        "            scores=results.boxes.conf.cpu().numpy()\n",
        "\n",
        "            # Draw and write frames\n",
        "            frame_processed = self.vis_track.draw_bounding_boxes(frame, boxes, ids, self.names, scores)\n",
        "\n",
        "            # draw fps\n",
        "            end_time=time.time()\n",
        "            fps=1/(end_time-start_time)\n",
        "            cv2.putText(frame_processed, f\"{fps:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "            # display\n",
        "            cv2.imshow(\"Webcam Feed\", frame_processed)\n",
        "\n",
        "            # Check for ESC key press\n",
        "            if cv2.waitKey(1) & 0xFF == 27:  # 27 is the ASCII code for ESC\n",
        "                print(\"ESC pressed, exiting...\")\n",
        "                break\n",
        "\n",
        "\n",
        "        cap.release()\n",
        "        cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOKXgqbTpYwC",
        "outputId": "2c6823c7-b5b3-443b-9e4c-9290bd07e76a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING ‚ö†Ô∏è Unable to automatically guess model task, assuming 'task=detect'. Explicitly define task for your model, i.e. 'task=detect', 'segment', 'classify','pose' or 'obb'.\n",
            "Processing 184 frames | Resolution: 1904x1070\n",
            "Loading yolo_11l_469.engine for TensorRT inference...\n",
            "Process finished in 0.13 minutes\n"
          ]
        }
      ],
      "source": [
        "inference=InferenceBaseline(\"yolo_11l_469.engine\")\n",
        "inference.inference_video(video_path=\"raw.mp4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5qEkSX2uUZw",
        "outputId": "d546ea57-81f9-44bf-dd30-f0ac966a38e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "YOLO11l summary (fused): 464 layers, 25,287,022 parameters, 0 gradients, 86.6 GFLOPs\n",
            "Processing 184 frames | Resolution: 1904x1070\n",
            "Process finished in 0.18 minutes\n"
          ]
        }
      ],
      "source": [
        "inference=InferenceBaseline(\"yolo_11l_469.pt\")\n",
        "inference.inference_video(video_path=\"raw.mp4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vx5RmRNEywd5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "GNN_M1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
